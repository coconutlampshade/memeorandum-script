#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import anthropic
import os
from urllib.parse import urlparse

# Scrape memeorandum river
url = "https://www.memeorandum.com/river"
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'}
r = requests.get(url, headers=headers)
soup = BeautifulSoup(r.text, "html.parser")

# Extract headlines (skip source-only links)
articles = []
seen_urls = set()

for a in soup.select('a[href]'):
    href = a.get('href', '')
    text = a.get_text(strip=True)

    # Filter for actual article links (not source names, not internal links)
    if (text and
        len(text) > 30 and  # Headlines are longer than source names
        href.startswith('http') and
        'memeorandum' not in href and
        href not in seen_urls):

        seen_urls.add(href)
        articles.append({"title": text, "url": href})

        if len(articles) >= 20:
            break

print("Pick the ones you want posts about:\n")
for i, article in enumerate(articles, 1):
    print(f"{i}. {article['title'][:100]}")
    print(f"   {article['url']}\n")

# Get user selection
selection = input("Enter article numbers (comma-separated, e.g., 1,3,5): ").strip()
selected_indices = [int(x.strip()) - 1 for x in selection.split(',') if x.strip().isdigit()]

# Filter selected articles
selected_articles = [articles[i] for i in selected_indices if 0 <= i < len(articles)]

if not selected_articles:
    print("No valid articles selected.")
    exit()

# Initialize Claude API
api_key = os.environ.get('ANTHROPIC_API_KEY')
if not api_key:
    print("Error: ANTHROPIC_API_KEY environment variable not set")
    exit(1)

client = anthropic.Anthropic(api_key=api_key)

# Process each selected article
for idx, article in enumerate(selected_articles, 1):
    print(f"\n{'='*80}")
    print(f"Generating post {idx}/{len(selected_articles)}")
    print(f"{'='*80}\n")

    # Fetch article content
    try:
        article_response = requests.get(article['url'], headers=headers, timeout=10)
        article_soup = BeautifulSoup(article_response.text, 'html.parser')

        # Extract main text content
        for tag in article_soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()

        article_text = article_soup.get_text(separator=' ', strip=True)[:5000]  # Limit to first 5000 chars

    except Exception as e:
        print(f"Warning: Could not fetch article content: {e}")
        article_text = article['title']

    # Extract source name from URL
    domain = urlparse(article['url']).netloc
    source_name = domain.replace('www.', '').split('.')[0].title()
    common_sources = {
        'nytimes': 'the New York Times',
        'washingtonpost': 'the Washington Post',
        'cnn': 'CNN',
        'bbc': 'BBC',
        'theguardian': 'the Guardian',
        'reuters': 'Reuters',
        'apnews': 'the Associated Press',
        'politico': 'Politico',
        'theatlantic': 'the Atlantic',
        'npr': 'NPR'
    }
    source_name = common_sources.get(domain.replace('www.', '').split('.')[0].lower(), source_name)

    # Generate Boingboing-style post with Claude
    prompt = f"""Based on this article, write a ~250 word blog post suitable for Boingboing.net. The post should:

1. Start with source attribution (e.g., "As reported in {source_name}, ...")
2. Be engaging and conversational in Boingboing's style
3. Be approximately 250 words
4. Focus on what's interesting or notable about the story

Article URL: {article['url']}
Article Title: {article['title']}
Article Content: {article_text}

After the post, provide 5 different headline options in sentence case (not title case). Each headline should be engaging and Boingboing-appropriate.

Format your response as:
POST:
[the 250-word post here]

HEADLINES:
1. [headline option 1]
2. [headline option 2]
3. [headline option 3]
4. [headline option 4]
5. [headline option 5]"""

    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1500,
        messages=[{"role": "user", "content": prompt}]
    )

    print(message.content[0].text)
    print(f"\nSource URL: {article['url']}")
    print(f"\n{'='*80}\n")
